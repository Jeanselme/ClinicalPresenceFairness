{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the models obtain previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pd.read_csv('data/labs_1_day.csv', index_col = [0, 1], header = [0, 1])\n",
    "outcomes = pd.read_csv('data/outcomes_1_day.csv', index_col = 0)\n",
    "outcomes['Death'] = ~outcomes.Death.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'ethnicity' # gender or ethnicity\n",
    "ethnicity = (outcomes.ETHNICITY == 'WHITE').replace({True: 'White', False: 'Non white'}) \n",
    "gender = (outcomes.GENDER == 'M').replace({True: 'Male', False: 'Female'})\n",
    "if group_name == 'ethnicity':\n",
    "    groups = ethnicity\n",
    "elif group_name == 'gender':\n",
    "    groups = gender\n",
    "groups_unique = groups.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = 'results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    'classification_reg_LOCF': 'LOCF',\n",
    "    'classification_reg_Individual': 'Individual Median',\n",
    "    'classification_reg_MICE': 'Population MICE',\n",
    "    'classification_reg_gender_specific': 'Gender MICE',\n",
    "    'classification_reg_ethnicity_specific': 'Ethnicity MICE',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for file in sorted(os.listdir(results)):\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "    name = file[:file.index('.csv')]\n",
    "    predictions[names[name]] = pd.read_csv(results + file, index_col=0)\n",
    "    print(file, ' -> ', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencesin observed labels between training and testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all metrics on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, groups, y_pred, iterations = 100):\n",
    "    \"\"\"\n",
    "        Compute boostrapped performances\n",
    "    \"\"\"\n",
    "    groups_unique = np.unique(groups).tolist() + [\"Overall\"]\n",
    "    fprs, tprs, rocs, brs = {b: [] for b in groups_unique}, {b: [] for b in groups_unique}, \\\n",
    "        {b: [] for b in groups_unique}, {b: [] for b in groups_unique}\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    fpr_sort = np.argsort(fpr)\n",
    "    tpr_sort = np.argsort(tpr)\n",
    "    threshold_fpr = np.interp(0.9, tpr[tpr_sort], thresholds[tpr_sort])\n",
    "    threshold_tpr = np.interp(0.1, fpr[fpr_sort], thresholds[fpr_sort])\n",
    "\n",
    "    for group in groups_unique:\n",
    "        if group == 'Overall':\n",
    "            y_pred_group = y_pred\n",
    "            y_true_group = y_true\n",
    "        else:\n",
    "            y_pred_group = y_pred[groups == group]\n",
    "            y_true_group = y_true[groups == group]\n",
    "        for i in range(iterations):\n",
    "            bootstrap = np.random.choice(np.arange(len(y_pred_group)), size = len(y_pred_group), replace = True) \n",
    "            y_pred_iteration = y_pred_group[bootstrap]\n",
    "            y_true_group_iter = y_true_group[bootstrap]\n",
    "\n",
    "            brs[group].append(brier_score_loss(y_true_group_iter, y_pred_iteration))\n",
    "            fpr, tpr, thresholds = roc_curve(y_true_group_iter, y_pred_iteration)\n",
    "            thres_order = np.argsort(thresholds)\n",
    "            fprs[group].append(np.interp(threshold_fpr, thresholds[thres_order], fpr[thres_order]))\n",
    "            tprs[group].append(np.interp(threshold_tpr, thresholds[thres_order], tpr[thres_order]))\n",
    "            rocs[group].append(roc_auc_score(y_true_group_iter, y_pred_iteration))\n",
    "\n",
    "    difference = 'Difference {} - {}'.format(groups_unique[0], groups_unique[1])\n",
    "    result = {\n",
    "        (difference, \"Brier Score\", 'Mean'): np.mean(np.array(brs[groups_unique[0]]) - np.array(brs[groups_unique[1]])),\n",
    "        (difference, \"Brier Score\", 'Std'): np.std(np.array(brs[groups_unique[0]]) - np.array(brs[groups_unique[1]])),\n",
    "        (difference, \"AUC ROC\", 'Mean'): np.mean(np.array(rocs[groups_unique[0]]) - np.array(rocs[groups_unique[1]])),\n",
    "        (difference, \"AUC ROC\", 'Std'): np.std(np.array(rocs[groups_unique[0]]) - np.array(rocs[groups_unique[1]])),\n",
    "        (difference, \"FPR @ 90% TPR\", 'Mean'): np.mean(np.array(fprs[groups_unique[0]]) - np.array(fprs[groups_unique[1]])),\n",
    "        (difference, \"FPR @ 90% TPR\", 'Std'): np.std(np.array(fprs[groups_unique[0]]) - np.array(fprs[groups_unique[1]])),\n",
    "        (difference, \"TPR @ 10% FPR\", 'Mean'): np.mean(np.array(tprs[groups_unique[0]]) - np.array(tprs[groups_unique[1]])),\n",
    "        (difference, \"TPR @ 10% FPR\", 'Std'): np.std(np.array(tprs[groups_unique[0]]) - np.array(tprs[groups_unique[1]])),\n",
    "    }\n",
    "    for group in groups_unique:\n",
    "        result.update({\n",
    "            (group, \"Brier Score\", 'Mean'): np.mean(brs[group]),\n",
    "            (group, \"Brier Score\", 'Std'): np.std(brs[group]),\n",
    "            (group, \"AUC ROC\", 'Mean'): np.mean(rocs[group]),\n",
    "            (group, \"AUC ROC\", 'Std'): np.std(rocs[group]),\n",
    "            (group, \"FPR @ 90% TPR\", 'Mean'): np.mean(fprs[group]),\n",
    "            (group, \"FPR @ 90% TPR\", 'Std'): np.std(fprs[group]),\n",
    "            (group, \"TPR @ 10% FPR\", 'Mean'): np.mean(tprs[group]),\n",
    "            (group, \"TPR @ 10% FPR\", 'Std'): np.std(tprs[group]),\n",
    "        })\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and display performances per group of model\n",
    "performances = {}\n",
    "for m in predictions:\n",
    "    print('-' * 42)\n",
    "    print(m)\n",
    "    performances[m] = {}\n",
    "\n",
    "    np.random.seed(42)\n",
    "    preds = predictions[m]\n",
    "\n",
    "    test = preds.Use != 'Train' # Use the data that will be used for both   \n",
    "    test = test[test].index\n",
    "    \n",
    "    performances[m] = evaluate(outcomes.Death.loc[test].values, groups.loc[test].values, preds.loc[test]['1'].values)\n",
    "performances = pd.concat(performances, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = performances.loc[:, ~performances.columns.get_level_values(0).str.contains('Difference')] # Ignore difference\n",
    "col = perf.columns.get_level_values(1)\n",
    "order = performances['Overall']['AUC ROC'][\"Mean\"].sort_values().index\n",
    "for metric in col.unique():\n",
    "    perf_metric = perf.loc[:, col == metric].droplevel(1, 1)\n",
    "    perf_metric_mean = perf_metric.loc[:, perf_metric.columns.get_level_values(1) == \"Mean\"].droplevel(1, 1)\n",
    "    perf_metric_std = 1.96 * perf_metric.loc[:, perf_metric.columns.get_level_values(1) == \"Std\"].droplevel(1, 1) / np.sqrt(100)\n",
    "\n",
    "    perf_metric_mean = perf_metric_mean.loc[order]\n",
    "    perf_metric_std = perf_metric_std.loc[order]\n",
    "    ax = perf_metric_mean.T.plot.barh(yerr = perf_metric_std.T)\n",
    "\n",
    "    plt.title('{}'.format(metric))\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xlabel('Performance')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(reversed(handles), reversed(labels), loc='center left', bbox_to_anchor=(1, 0.5), )\n",
    "\n",
    "    if 'ROC' in metric:\n",
    "        plt.xlim(0.6, 0.8)\n",
    "    elif 'rier' in metric:\n",
    "        plt.xlim(0.10, 0.14)\n",
    "    else:\n",
    "        plt.xlim(0., 1.1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = performances.loc[:, performances.columns.get_level_values(0).str.contains('Difference')].droplevel(0, 1)\n",
    "col = perf.columns.get_level_values(0)\n",
    "order = performances['Overall']['AUC ROC'][\"Mean\"].sort_values().index\n",
    "for metric in col.unique():\n",
    "    perf_metric = perf.loc[:, col == metric]\n",
    "    perf_metric = perf_metric.loc[:, perf_metric.columns.get_level_values(1) == \"Mean\"].droplevel(1, 1)\n",
    "    perf_metric.loc[order].plot.barh(legend = False)\n",
    "\n",
    "    plt.title('{} difference'.format(metric))\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.xlabel('Performance {} - {}'.format(groups_unique[0], groups_unique[1]))\n",
    "    plt.xlim(-0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a54f3b3a447186e9a4a83057d2abe8df010acd7b8f131225203d307ef84eba48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Jupyter': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
