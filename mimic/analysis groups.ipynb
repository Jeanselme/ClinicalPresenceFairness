{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to compare the pipelines obtained following the different imputation strategies on the MIMIC dataset. **This notebook requires `experiment.ipynb` to have been run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen data\n",
    "labs = pd.read_csv('data/labs_1_day.csv', index_col = [0, 1], header = [0, 1])\n",
    "outcomes = pd.read_csv('data/outcomes_1_day.csv', index_col = 0)\n",
    "outcomes['Death'] = outcomes.Death < 8 # Define binary outcome of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define group of interest\n",
    "ethnicity = outcomes.ETHNICITY.str.contains('BLACK').replace({True: 'Black', False: 'Non Black'}) \n",
    "ethnicity_unique = ['Black', 'Non Black']\n",
    "\n",
    "gender = (outcomes.GENDER == 'M').replace({True: 'Male', False: 'Female'})\n",
    "gender_unique = ['Female', 'Male']\n",
    "\n",
    "insurance = (outcomes.INSURANCE == 'Private').replace({True: 'Private', False: 'Public'})\n",
    "insurance_unique = ['Public', 'Private'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the venn diagram\n",
    "(ethnicity + ' + ' + gender + ' + ' + insurance).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = 'results/'\n",
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_all = True # Is the group imputation taking all group or not into account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "    file: file[file.rindex('classification')+14:file.rindex('.csv')]\n",
    "    for file in os.listdir('results') if '.csv' in file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "\n",
    "for file in names:\n",
    "    predictions[names[file]] = pd.read_csv(results + file, index_col=0)\n",
    "    print(file, ' -> ', names[file])\n",
    "\n",
    "names_reformat = ['Mean'] # Select methods to comapre\n",
    "names_reformat += ['Group {} Mean'.format(g) for g in ['Ethnicity', 'Sex', 'Insurance', 'All']]\n",
    "names_reformat += ['MICE']\n",
    "names_reformat += ['Group {} MICE'.format(g) for g in ['Ethnicity', 'Sex', 'Insurance', 'All']]\n",
    "names_reformat += ['Group {} MICE Missing'.format(g) for g in ['Ethnicity', 'Sex', 'Insurance', 'All']]\n",
    "predictions = {name: predictions[name] for name in names_reformat[::-1]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, brier_score_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencesin observed labels between training and testing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate all metrics on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, groups, groups_unique, y_pred, iterations = 100, p = threshold):\n",
    "    \"\"\"\n",
    "        Compute boostrapped performances\n",
    "    \"\"\"\n",
    "    fprs, tprs, rocs, brs, screened, screened_fpr, screened_fnr = {b: [] for b in groups_unique}, {b: [] for b in groups_unique}, \\\n",
    "        {b: [] for b in groups_unique}, {b: [] for b in groups_unique}, {b: [] for b in groups_unique}, {b: [] for b in groups_unique}, {b: [] for b in groups_unique}\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    fpr_sort = np.argsort(fpr)\n",
    "    tpr_sort = np.argsort(tpr)\n",
    "    threshold_fpr = np.interp(0.9, tpr[tpr_sort], thresholds[tpr_sort])\n",
    "    threshold_tpr = np.interp(0.1, fpr[fpr_sort], thresholds[fpr_sort])\n",
    "\n",
    "    # Threshold screening\n",
    "    threshold_top = pd.Series(y_pred).nlargest(int(len(y_pred) * p), keep = 'all').min()\n",
    "\n",
    "    for group in groups_unique:\n",
    "        if group == 'Overall':\n",
    "            y_pred_group = y_pred\n",
    "            y_true_group = y_true\n",
    "        else:\n",
    "            y_pred_group = y_pred[groups == group]\n",
    "            y_true_group = y_true[groups == group]\n",
    "\n",
    "        for i in range(iterations):\n",
    "            bootstrap = np.random.choice(np.arange(len(y_pred_group)), size = len(y_pred_group), replace = True) \n",
    "            y_pred_iteration = y_pred_group[bootstrap]\n",
    "            y_true_iteration = y_true_group[bootstrap]\n",
    "\n",
    "            # Standard metrics on the boostrapped sample\n",
    "            brs[group].append(brier_score_loss(y_true_iteration, y_pred_iteration))\n",
    "            fpr, tpr, thresholds = roc_curve(y_true_iteration, y_pred_iteration)\n",
    "            thres_order = np.argsort(thresholds)\n",
    "            fprs[group].append(np.interp(threshold_fpr, thresholds[thres_order], fpr[thres_order]))\n",
    "            tprs[group].append(np.interp(threshold_tpr, thresholds[thres_order], tpr[thres_order]))\n",
    "            rocs[group].append(roc_auc_score(y_true_iteration, y_pred_iteration))\n",
    "\n",
    "            # Percentage screened-out in bottom 30 %\n",
    "            selected = y_pred_iteration >= threshold_top\n",
    "            screened[group].append(np.mean(selected)) # Percentage of patients in this group that are prioritized\n",
    "            screened_fnr[group].append((y_true_iteration[~selected]).sum() / y_true_iteration.sum()) # Wrongly not prioritized\n",
    "            screened_fpr[group].append((1 - y_true_iteration[selected]).sum() / (1 - y_true_iteration).sum()) # Wrongly prioritized\n",
    "\n",
    "    result = {}\n",
    "    if len(groups_unique) == 2:\n",
    "        difference = 'Difference {} - {}'.format(groups_unique[0], groups_unique[1])\n",
    "        result.update({\n",
    "            (difference, \"Brier Score\", 'Mean'): np.mean(np.array(brs[groups_unique[0]]) - np.array(brs[groups_unique[1]])),\n",
    "            (difference, \"Brier Score\", 'Std'): np.std(np.array(brs[groups_unique[0]]) - np.array(brs[groups_unique[1]])),\n",
    "            (difference, \"AUC ROC\", 'Mean'): np.mean(np.array(rocs[groups_unique[0]]) - np.array(rocs[groups_unique[1]])),\n",
    "            (difference, \"AUC ROC\", 'Std'): np.std(np.array(rocs[groups_unique[0]]) - np.array(rocs[groups_unique[1]])),\n",
    "\n",
    "            (difference, \"FPR @ 90% TPR\", 'Mean'): np.mean(np.array(fprs[groups_unique[0]]) - np.array(fprs[groups_unique[1]])),\n",
    "            (difference, \"FPR @ 90% TPR\", 'Std'): np.std(np.array(fprs[groups_unique[0]]) - np.array(fprs[groups_unique[1]])),\n",
    "            (difference, \"TPR @ 10% FPR\", 'Mean'): np.mean(np.array(tprs[groups_unique[0]]) - np.array(tprs[groups_unique[1]])),\n",
    "            (difference, \"TPR @ 10% FPR\", 'Std'): np.std(np.array(tprs[groups_unique[0]]) - np.array(tprs[groups_unique[1]])),\n",
    "\n",
    "            (difference, \"Prioritized\", 'Mean'): np.mean(np.array(screened[groups_unique[0]]) - np.array(screened[groups_unique[1]])),\n",
    "            (difference, \"Prioritized\", 'Std'): np.std(np.array(screened[groups_unique[0]]) - np.array(screened[groups_unique[1]])),\n",
    "            (difference, \"Wrongly prioritized (FPR)\", 'Mean'): np.mean(np.array(screened_fpr[groups_unique[0]]) - np.array(screened_fpr[groups_unique[1]])),\n",
    "            (difference, \"Wrongly prioritized (FPR)\", 'Std'): np.std(np.array(screened_fpr[groups_unique[0]]) - np.array(screened_fpr[groups_unique[1]])),\n",
    "            (difference, \"Wrongly not prioritized (FNR)\", 'Mean'): np.mean(np.array(screened_fnr[groups_unique[0]]) - np.array(screened_fnr[groups_unique[1]])),\n",
    "            (difference, \"Wrongly not prioritized (FNR)\", 'Std'): np.std(np.array(screened_fnr[groups_unique[0]]) - np.array(screened_fnr[groups_unique[1]])),\n",
    "        })\n",
    "    for group in groups_unique:\n",
    "        result.update({\n",
    "            (group, \"Brier Score\", 'Mean'): np.mean(brs[group]),\n",
    "            (group, \"Brier Score\", 'Std'): np.std(brs[group]),\n",
    "            (group, \"AUC ROC\", 'Mean'): np.mean(rocs[group]),\n",
    "            (group, \"AUC ROC\", 'Std'): np.std(rocs[group]),\n",
    "\n",
    "            (group, \"FPR @ 90% TPR\", 'Mean'): np.mean(fprs[group]),\n",
    "            (group, \"FPR @ 90% TPR\", 'Std'): np.std(fprs[group]),\n",
    "            (group, \"TPR @ 10% FPR\", 'Mean'): np.mean(tprs[group]),\n",
    "            (group, \"TPR @ 10% FPR\", 'Std'): np.std(tprs[group]),\n",
    "\n",
    "            (group, \"Prioritized\", 'Mean'): np.mean(screened[group]),\n",
    "            (group, \"Prioritized\", 'Std'): np.std(screened[group]),\n",
    "            (group, \"Wrongly prioritized (FPR)\", 'Mean'): np.mean(screened_fpr[group]),\n",
    "            (group, \"Wrongly prioritized (FPR)\", 'Std'): np.std(screened_fpr[group]),\n",
    "            (group, \"Wrongly not prioritized (FNR)\", 'Mean'): np.mean(screened_fnr[group]),\n",
    "            (group, \"Wrongly not prioritized (FNR)\", 'Std'): np.std(screened_fnr[group]),\n",
    "        })\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute and display performances per group of model\n",
    "performances = {}\n",
    "for groups, groups_unique, group_name in [(gender, gender_unique, 'Sex'), (ethnicity, ethnicity_unique, 'Ethnicity'), (insurance, insurance_unique, 'Insurance'), (outcomes, ['Overall'], 'Overall')]:\n",
    "    print('-' * 42)\n",
    "    print('Computing for group: ', group_name)\n",
    "    perf_group = {}\n",
    "    for m in predictions:\n",
    "\n",
    "        if (group_name != 'Overall'):\n",
    "            if group_all and (('Group' in m) and not('All' in m)): \n",
    "                continue\n",
    "            elif not(group_all) and (('Group' in m) and not(group_name in m)):\n",
    "                continue\n",
    "\n",
    "        print('\\t- ', m)\n",
    "\n",
    "        np.random.seed(42)\n",
    "        preds = predictions[m]\n",
    "\n",
    "        test = preds.Use != 'Train' # Use the data that will be used for both   \n",
    "        test = test[test].index\n",
    "\n",
    "        if (group_name != 'Overall') and ('Group' in m):\n",
    "            # Rename method to explore group specific mean\n",
    "            m = 'Group Mean{}'.format(' Missing' if 'Miss' in m else '') if 'Mean' in m else 'Group MICE{}'.format(' Missing' if 'Miss' in m else '')\n",
    "\n",
    "        if m == 'Mean':\n",
    "            m = 'Population Mean'\n",
    "        \n",
    "        perf_group[m] = evaluate(outcomes.Death.loc[test].values, groups.loc[test].values, groups_unique, preds.loc[test]['Mean'].values)\n",
    "    performances[group_name] = pd.concat(perf_group, axis = 1).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"AUC ROC\" #'Prioritized', 'AUC ROC', 'Wrongly not prioritized (FNR)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for groups, groups_unique, group_name in [(gender, gender_unique, 'Sex'), (ethnicity, ethnicity_unique, 'Ethnicity'), (insurance, insurance_unique, 'Insurance'), (outcomes, ['Overall'], 'Overall')]:\n",
    "    perf_group = performances[group_name][groups_unique]\n",
    "    perf_group = perf_group.loc[:, perf_group.columns.get_level_values(1) == metric].droplevel(1, 1)\n",
    "    perf_group = pd.DataFrame.from_dict({model: [\"{:.3f} ({:.3f})\".format(perf_group.loc[model].loc[i].Mean, perf_group.loc[model].loc[i].Std) for i in perf_group.loc[model].index.get_level_values(0).unique()] for model in perf_group.index}, columns = perf_group.columns.get_level_values(0).unique(), orient = 'index')\n",
    "    print(perf_group.T.to_latex())\n",
    "perf_group.T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference in FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {}\n",
    "for model in performances['Sex'].index[::-1]:\n",
    "    comparison[model] = pd.concat({\n",
    "        'Insurance': performances['Insurance'].loc[model]['Difference Public - Private'][metric],\n",
    "        'Sex': performances['Sex'].loc[model]['Difference Female - Male'][metric],\n",
    "        'Ethnicity': performances['Ethnicity'].loc[model]['Difference Black - Non Black'][metric],\n",
    "    }, axis = 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_short = {\n",
    "    \"Brier Score\": \"Brier\",\n",
    "    \"AUC ROC\": \"AUC\",\n",
    "    \"FPR @ 90% TPR\": \"False Positive Rate\",\n",
    "    \"TPR @ 10% FPR\": \"True Positive Rate\",\n",
    "    \"Prioritized\": \"Prioritisation\",\n",
    "    \"Wrongly prioritized (FPR)\": \"False Positive Rate\",\n",
    "    \"Wrongly not prioritized (FNR)\": \"False Negative Rate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.concat(comparison, axis = 1).swaplevel(0, axis = 1)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = comparison.Mean.plot.barh(xerr = 1.96 * comparison.Std / np.sqrt(100), width = 0.7, legend = 'FNR' in metric, figsize = (6.4, 4.8))\n",
    "hatches = ['', 'ooo', 'xx', '//', '||', '***', '++']\n",
    "for i, thisbar in enumerate(ax.patches):\n",
    "    c = list(plt_colors.to_rgba('tab:blue'))\n",
    "    c[3] = 0.35 if i // len(comparison) in [0,2] else 1\n",
    "    thisbar.set(edgecolor = '#eaeaf2', facecolor = c, linewidth = 1, hatch = hatches[i // len(comparison)])\n",
    "\n",
    "if 'FNR' in metric:\n",
    "    patches = [ax.patches[i * len(comparison)] for i in range(len(comparison.Mean.columns))][::-1]\n",
    "    labels = comparison.Mean.columns.tolist()[::-1]\n",
    "\n",
    "    ax.legend(patches, labels, loc='upper left', bbox_to_anchor=(1.04, 1.04), frameon=False,\n",
    "        handletextpad = 0.5, handlelength = 1.0, columnspacing = -0.5,)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "if 'AUC' in metric:\n",
    "    plt.xlim(-0.11, 0.12)\n",
    "else:\n",
    "    plt.xlim(-0.45, 0.45)\n",
    "plt.axvline(0, ls = '--', alpha = 0.5, c = 'k')\n",
    "plt.xlabel('$\\Delta$ {}'.format(metrics_short[metric]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame.from_dict({group: [\"{:.3f} ({:.3f})\".format(comparison.loc[group].loc[('Mean', i)], comparison.loc[group].loc[('Std', i)]) for i in comparison.loc[group].index.get_level_values(1).unique()] for group in comparison.index}, columns = comparison.columns.get_level_values(1).unique(), orient = 'index').to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
