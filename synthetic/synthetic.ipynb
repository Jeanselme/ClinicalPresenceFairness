{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to reproduce the paper synthetic results. First, we create a synthetic population with different disease expression. Then, we enforce missingness following three scenario of clinical presence, i.e. the interaction between patient and the healthcare system:\n",
    "- (Mis)-informed collection\n",
    "- Limited access to quality care\n",
    "- Confirmation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random repetitions\n",
    "k = 100\n",
    "\n",
    "# Data distribution\n",
    "points = 100000 # Number points for the majority\n",
    "ratio = 0.01 # Proportion of minority\n",
    "class_balance = 0.66 # Class balance for positive and negatives (for paper, if generate_data_linear_shift then 0.66) [0.1, 0.5]\n",
    "\n",
    "generate = generate_data_linear_shift # How to generate the data: generate_data_linear_shift or generate_data_same or generate_data_linear_corr_shift\n",
    "frontier = 0.5 # Where to put the threshold on missingness for S1 and S2 (for paper, if generate_data_linear_shift then 0.5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the data from three gaussian: one for positives and two for negatives (one for the minority and one for the majority). This same function is then called at each $k$ iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, protected_binarized, protected = generate(majority_size = points, ratio = ratio)\n",
    "display_data(data, labels, protected, distribution = True, legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputations strategies to explore\n",
    "imputations = {\n",
    "                'Population Mean': {'strategy': 'Mean'},\n",
    "                'Group Mean': {'strategy': 'Group Mean'},\n",
    "                \n",
    "                'MICE': {'strategy': 'MICE'},\n",
    "                'Group MICE': {'strategy': 'Group MICE'},\n",
    "                'Group MICE Missing': {'strategy': 'Group MICE', 'add_missing': True}, \n",
    "              }\n",
    "alphas = [0.35, 1, 0.35, 1, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited access to quality care"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Care is more limited in the marginalised group. Missingness is therefore concentrated in this group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_access(data, labels, protected, seed = 42):\n",
    "    p = (protected == \"Minority\").astype(float) # All minority\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index # 50 % missing\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_lim, reconstruction_lim = {}, {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_lim[name], coefs, (reconstruction_lim[name], mean_observed_lim, obs_rate_lim, corr_lim, corr_cov), imputed = k_experiment(majority_size = points, ratio = ratio, class_balance = class_balance, \n",
    "            generate = generate, removal = limited_access, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_lim, alphas = alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_lim, 'Brier Score', legend = False, alphas = alphas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mis-informed collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingness is informed by the standard guidelines. We propose that the first dimension is observed only if the second is in a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misinformed(data, labels, groups, seed = 42): # Must respect this signature\n",
    "    p = (data.iloc[:, 1] > frontier).astype(float) # All above threshold\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_mis, reconstruction_mis = {}, {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_mis[name], coefs, (reconstruction_mis[name], mean_observed_mis, obs_rate_mis, corr_mis, corr_cov), imputed= k_experiment(majority_size = points, ratio = ratio, class_balance = class_balance, \n",
    "            generate = generate, removal = misinformed, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_mis, alphas = alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_mis, 'Brier Score', legend = False, alphas = alphas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmation bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test is performed when the outcome is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirmation(data, labels, protected, seed = 42):\n",
    "    p = (data.iloc[:, 0] > frontier).astype(float) # All negatives\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index # 50 % missing\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_conf, reconstruction_conf = {}, {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_conf[name], coefs, (reconstruction_conf[name], mean_observed_conf, obs_rate_conf, corr_conf, corr_cov), imputed = k_experiment(majority_size = points, ratio = ratio, class_balance = class_balance, \n",
    "            generate = generate, removal = confirmation, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_conf, alphas = alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_conf, 'Brier Score', alphas = alphas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison minority groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following functions allow to reproduce the table and plots presented in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_minority = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation \\n bias (S3)\": performance_conf[m]['Minority'],\n",
    "        \"(Mis)-Informed \\n collection (S2)\": performance_mis[m]['Minority'],\n",
    "        \"Limited access \\n to quality care (S1)\": performance_lim[m]['Minority'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}\n",
    "\n",
    "performances_majority = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation \\n bias (S3)\": performance_conf[m]['Majority'],\n",
    "        \"(Mis)-Informed \\n collection (S2)\": performance_mis[m]['Majority'],\n",
    "        \"Limited access \\n to quality care (S1)\": performance_lim[m]['Majority'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}\n",
    "\n",
    "performances_overall = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation \\n bias (S3)\": performance_conf[m]['Overall'],\n",
    "        \"(Mis)-Informed \\n collection (S2)\": performance_mis[m]['Overall'],\n",
    "        \"Limited access \\n to quality care (S1)\": performance_lim[m]['Overall'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUC'\n",
    "\n",
    "difference = {\n",
    "    imput: pd.concat({'Mean': (performances_minority[imput][performances_minority[imput].index.get_level_values(1) == metric] - performances_majority[imput][performances_minority[imput].index.get_level_values(1) == metric]).mean(),\n",
    "            'Std': (performances_minority[imput][performances_minority[imput].index.get_level_values(1) == metric] - performances_majority[imput][performances_minority[imput].index.get_level_values(1) == metric]).std()}, axis = 1)\n",
    "    for imput in performances_overall\n",
    "}\n",
    "\n",
    "difference = pd.concat(difference, axis = 1)\n",
    "difference = difference.swaplevel(0, axis = 1)\n",
    "print_pandas_latex(difference['Mean'], difference['Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.concat({\n",
    "            \"Confirmation \\n bias (S3)\": mean_observed_conf,\n",
    "            \"(Mis)-Informed \\n collection (S2)\": mean_observed_mis,\n",
    "            \"Limited access \\n to quality care (S1)\": mean_observed_lim,\n",
    "        }, axis = 1)\n",
    "\n",
    "obs = pd.concat({\n",
    "            \"Confirmation \\n bias (S3)\": obs_rate_conf,\n",
    "            \"(Mis)-Informed \\n collection (S2)\": obs_rate_mis,\n",
    "            \"Limited access \\n to quality care (S1)\": obs_rate_lim,\n",
    "        }, axis = 1)\n",
    "\n",
    "corr = pd.concat({\n",
    "            \"Confirmation \\n bias (S3)\": corr_conf,\n",
    "            \"(Mis)-Informed \\n collection (S2)\": corr_mis,\n",
    "            \"Limited access \\n to quality care (S1)\": corr_lim,\n",
    "        }, axis = 1)\n",
    "\n",
    "error = {imputation: pd.concat({\n",
    "            \"Confirmation \\n bias (S3)\": reconstruction_conf[imputation],\n",
    "            \"(Mis)-Informed \\n collection (S2)\": reconstruction_mis[imputation],\n",
    "            \"Limited access \\n to quality care (S1)\": reconstruction_lim[imputation],\n",
    "        }, axis = 1)\n",
    "        for imputation in imputations}\n",
    "error = pd.concat(error)\n",
    "error = error.swaplevel(0, 1, axis = 0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pandas_latex(error['Mean'], error['Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pandas_latex(means.loc['Mean'].T, means.loc['Std'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pandas_latex(obs.loc['Mean'].T, obs.loc['Std'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pandas_latex(corr.loc['Mean'].T, corr.loc['Std'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display delta performance\n",
    "difference_error = error.loc[:, error.columns.get_level_values(2) == 'Difference'].droplevel(2, 'columns')\n",
    "ax1 = difference_error.Mean.plot.barh(xerr = 1.96 * difference_error.Std / np.sqrt(k), width = 0.7, figsize = (6.4, 4.8))\n",
    "hatches = ['', 'ooo', 'xx', '//', '||', '***', '++'] * 2\n",
    "for i, thisbar in enumerate(ax1.patches):\n",
    "    c = list(plt_colors.to_rgba('tab:blue'))\n",
    "    c[3] = 0.35 if i // len(difference_error) in [0,2] else 1\n",
    "    thisbar.set(edgecolor = '#eaeaf2', facecolor = c, linewidth = 1, hatch = hatches[i // len(difference_error)])\n",
    "\n",
    "patches = [ax1.patches[i * len(difference_error)] for i in range(len(difference_error.Mean.columns))][::-1]\n",
    "labels = difference_error.Mean.columns.tolist()[::-1]\n",
    "\n",
    "ax1.legend([], [], framealpha = 0)\n",
    "plt.axvline(0, ls = '--', alpha = 0.5, c = 'k')\n",
    "plt.xlabel('$\\Delta$ Reconstruction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, last = None, 0\n",
    "patches += ax1.plot(np.NaN, np.NaN, '-', color='none')\n",
    "labels += [' ']\n",
    "for (color, group, name) in [('tab:orange', 'Majority', 'Majority'), ('tab:blue', 'Minority', 'Marginalised'), ('tab:gray', 'Overall', 'Overall')]:\n",
    "    mean = error.loc[:, error.columns.get_level_values(2) == group].droplevel(2, 'columns')\n",
    "    ax = mean.Mean.plot.barh(ax = ax, legend = False, xerr = 1.96 * difference.Std / np.sqrt(100), width = 0.7, ecolor = color, error_kw = {\"alpha\": 0.25, 'elinewidth': 3}, figsize = (3.2, 4.8))\n",
    "\n",
    "    # Remove bar and replace with dot\n",
    "    for i, thisbar in enumerate(ax.patches):\n",
    "        if i >= last:\n",
    "            thisbar.set(alpha = 0)\n",
    "            dot = ax.scatter(thisbar.get_width(), thisbar.get_y() + thisbar.get_height() / 2, alpha = 0.7,\n",
    "                          marker = ('|' if name != 'Overall' else 'x'), s = 125, color = color, linewidths=3)\n",
    "            last += 1\n",
    "\n",
    "    patches += [dot]\n",
    "    labels += [name]\n",
    "\n",
    "ax.set_yticklabels([' ', ' ', ' '])\n",
    "ax.legend(patches, labels, loc='upper left', bbox_to_anchor=(1.3, 1.04), frameon = False,\n",
    "        handletextpad = 0.5, handlelength = 1.0, columnspacing = -0.5,)\n",
    "ax.set_xlabel('Reconstruction Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = difference.Mean.plot.barh(xerr = 1.96 * difference.Std / np.sqrt(k), width = 0.7, figsize = (6.4, 4.8))\n",
    "hatches = ['', 'ooo', 'xx', '//', '||', '***', '++'] * 2\n",
    "for i, thisbar in enumerate(ax1.patches):\n",
    "    c = list(plt_colors.to_rgba('tab:blue'))\n",
    "    c[3] = 0.35 if i // len(difference) in [0,2] else 1\n",
    "    thisbar.set(edgecolor = '#eaeaf2', facecolor = c, linewidth = 1, hatch = hatches[i // len(difference)])\n",
    "\n",
    "# Destroy legend but keep for next\n",
    "patches = [ax1.patches[i * len(difference)] for i in range(len(difference.Mean.columns))][::-1]\n",
    "labels = difference.Mean.columns.tolist()[::-1]\n",
    "ax1.legend([], [], framealpha = 0)\n",
    "plt.xlim(-0.6, 0.02)\n",
    "plt.axvline(0, ls = '--', alpha = 0.5, c = 'k')\n",
    "plt.xlabel('$\\Delta$ AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, last = None, 0\n",
    "patches += ax1.plot(np.NaN, np.NaN, '-', color='none')\n",
    "labels += [' ']\n",
    "for (group, color, name) in [(performances_majority, 'tab:orange', 'Majority'), (performances_minority, 'tab:blue', 'Marginalised'), (performances_overall, 'tab:gray', 'Overall')]:\n",
    "    mean = {\n",
    "        imput: pd.concat({'Mean': group[imput][group[imput].index.get_level_values(1) == metric].mean(),\n",
    "            'Std': group[imput][group[imput].index.get_level_values(1) == metric].std()}, axis = 1)\n",
    "        for imput in performances_overall\n",
    "    }\n",
    "    mean = pd.concat(mean, axis = 1).swaplevel(0, axis = 1)\n",
    "    ax = mean.Mean.plot.barh(ax = ax, legend = False, xerr = 1.96 * difference.Std / np.sqrt(100), width = 0.7, ecolor = color, error_kw = {\"alpha\": 0.25, 'elinewidth': 3}, figsize = (3.2, 4.8))\n",
    "\n",
    "    # Remove bar and replace with dot\n",
    "    for i, thisbar in enumerate(ax.patches):\n",
    "        if i >= last:\n",
    "            thisbar.set(alpha = 0)\n",
    "            dot = ax.scatter(thisbar.get_width(), thisbar.get_y() + thisbar.get_height() / 2, alpha = 0.7,\n",
    "                          marker = ('|' if name != 'Overall' else 'x'), s = 125, color = color, linewidths=3)\n",
    "            last += 1\n",
    "\n",
    "    patches += [dot]\n",
    "    labels += [name]\n",
    "\n",
    "ax.set_yticklabels([' ', ' ', ' '])\n",
    "ax.set_xlim(0.35, 1.05)\n",
    "ax.legend(patches, labels, loc='upper left', bbox_to_anchor=(1.3, 1.04), frameon=False,\n",
    "        handletextpad = 0.5, handlelength = 1.0, columnspacing = -0.5,)\n",
    "ax.set_xlabel('Group-specific AUC')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
