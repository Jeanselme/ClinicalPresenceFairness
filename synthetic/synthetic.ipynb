{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to reproduce the paper synthetic results. First, we create a synthetic population with different disease expression. Then, we enforce missingness following three scenario of clinical presence, i.e. the interaction between patient and the healthcare system:\n",
    "- (Mis)-informed collection\n",
    "- Limited access to quality care\n",
    "- Confirmation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random repetitions\n",
    "k = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create the data from three gaussian: one for positives and two for negatives (one for the minority and one for the majority). This same function is then called at each $k$ iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, protected_binarized, protected = generate_data_linear_shift(majority_size = 10000, ratio = 0.01)\n",
    "display_data(data, labels, protected, distribution = True, legend = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputations strategies to explore\n",
    "imputations = {\n",
    "                #'Complete Case': {'strategy': 'Median', 'complete_case': True},\n",
    "                'Median': {'strategy': 'Median'},\n",
    "                # 'Mean': {'strategy': 'Mean'},\n",
    "                # 'Group Mean': {'strategy': 'Group Mean'},\n",
    "                'Median Missing': {'strategy': 'Median', 'add_missing': True},\n",
    "                'MICE': {'strategy': 'MICE'},\n",
    "                'MICE Missing': {'strategy': 'MICE', 'add_missing': True},\n",
    "                'Group MICE': {'strategy': 'Group MICE'},\n",
    "                'Group MICE Missing': {'strategy': 'Group MICE', 'add_missing': True}, \n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited access to quality care"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Care is more limited in the marginalised group. Missingness is therefore concentrated in this group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limited_access(data, labels, protected, seed = 42):\n",
    "    p = (protected == \"Minority\").astype(float) # All minority\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index # 50 % missing\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_lim = {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_lim[name], coefs, imputed = k_experiment(majority_size = 10000, ratio = 0.01, class_balance = 0.5, \n",
    "            removal = limited_access, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_lim, alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_lim, 'Brier Score', legend = False, alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mis-informed collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingness is informed by the standard guidelines. We propose that the first dimension is observed only if the second is in a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misinformed(data, labels, groups, seed = 42): # Must respect this signature\n",
    "    p = (data.iloc[:, 1] > 0.5).astype(float) # All above threshold\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_mis = {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_mis[name], coefs, imputed = k_experiment(majority_size = 10000, ratio = 0.01, class_balance = 0.5, \n",
    "            removal = misinformed, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_mis, alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_mis, 'Brier Score', legend = False, alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmation bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test is performed when the outcome is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirmation(data, labels, protected, seed = 42):\n",
    "    p = (data.iloc[:, 0] > 0.5).astype(float) # All negatives\n",
    "    total = p.sum()\n",
    "    selection = data.sample(int(total * 0.5), replace = False, weights = p / total, random_state = seed).index # 50 % missing\n",
    "    missing = data.copy()\n",
    "    missing.loc[selection, 0] = np.nan\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for results\n",
    "performance_conf = {}\n",
    "\n",
    "for name, args in imputations.items():\n",
    "    print(\"Computing: \", name)\n",
    "    ## Modelling\n",
    "    performance_conf[name], coefs, imputed = k_experiment(majority_size = 10000, ratio = 0.01, class_balance = 0.5, \n",
    "            removal = confirmation, k = k, n_imputation = 10 if 'MICE' in name else 1, **args)\n",
    "\n",
    "    ## Display\n",
    "    # data, imputed, labels, protected_binarized, protected = imputed\n",
    "    # display_data(imputed.Mean, labels, protected, distribution = True, legend = False)\n",
    "    # plt.scatter([], [], alpha = 0, label = ' ')\n",
    "    # plt.axline((0, coefs[0]), slope = coefs[1], c = 'black', ls = '-.', label = 'Decision boundary')\n",
    "    # if name == 'Group MICE':\n",
    "    #     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_conf, alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_result(performance_conf, 'Brier Score', alphas = [0.35, 0.35, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison minority groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following functions allow to reproduce the table and plots presented in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances_minority = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation bias (S3)\": performance_conf[m]['Minority'],\n",
    "        \"(Mis)-Informed collection (S2)\": performance_mis[m]['Minority'],\n",
    "        \"Limited access to quality care (S1)\": performance_lim[m]['Minority'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}\n",
    "\n",
    "performances_majority = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation bias (S3)\": performance_conf[m]['Majority'],\n",
    "        \"(Mis)-Informed collection (S2)\": performance_mis[m]['Majority'],\n",
    "        \"Limited access to quality care (S1)\": performance_lim[m]['Majority'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}\n",
    "\n",
    "performances_overall = {m:\n",
    "    pd.concat({\n",
    "        \"Confirmation bias (S3)\": performance_conf[m]['Overall'],\n",
    "        \"(Mis)-Informed collection (S2)\": performance_mis[m]['Overall'],\n",
    "        \"Limited access to quality care (S1)\": performance_lim[m]['Overall'],\n",
    "    }, axis = 1)\n",
    "for m in performance_lim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'AUC'\n",
    "\n",
    "difference = {\n",
    "    imput: pd.concat({'Mean': (performances_minority[imput][performances_minority[imput].index.get_level_values(1) == metric] - performances_majority[imput][performances_minority[imput].index.get_level_values(1) == metric]).mean(),\n",
    "            'Std': (performances_minority[imput][performances_minority[imput].index.get_level_values(1) == metric] - performances_majority[imput][performances_minority[imput].index.get_level_values(1) == metric]).std()}, axis = 1)\n",
    "    for imput in performances_overall\n",
    "}\n",
    "\n",
    "difference = pd.concat(difference, axis = 1)\n",
    "difference = difference.swaplevel(0, axis = 1)\n",
    "print(difference.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display delta performance\n",
    "\n",
    "ax1 = difference.Mean.plot.barh(xerr = 1.96 * difference.Std / np.sqrt(100), width = 0.7, figsize = (6.4, 4.8))\n",
    "hatches = ['', 'ooo', 'xx', '//', '||', '***', '++']\n",
    "for i, thisbar in enumerate(ax1.patches):\n",
    "    c = list(plt_colors.to_rgba('tab:blue'))\n",
    "    c[3] = 0.35 if i // len(difference) < 2 else 1\n",
    "    thisbar.set(edgecolor = '#eaeaf2', facecolor = c, linewidth = 1, hatch = hatches[i // len(difference)])\n",
    "\n",
    "# Destroy legend but keep for next\n",
    "patches = [ax1.patches[i * len(difference)] for i in range(len(difference.Mean.columns))][::-1]\n",
    "labels = difference.Mean.columns.tolist()[::-1]\n",
    "ax1.legend([], [], framealpha = 0)\n",
    "\n",
    "plt.xlim(-0.70, 0.05)\n",
    "plt.axvline(0, ls = '--', alpha = 0.5, c = 'k')\n",
    "plt.xlabel('$\\Delta$ AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, last = None, 0\n",
    "patches += ax1.plot(np.NaN, np.NaN, '-', color='none')\n",
    "labels += [' ']\n",
    "for (group, color, name) in [(performances_majority, 'tab:orange', 'Majority'), (performances_minority, 'tab:blue', 'Marginalized'), (performances_overall, 'tab:gray', 'Overall')]:\n",
    "    mean = {\n",
    "        imput: pd.concat({'Mean': group[imput][group[imput].index.get_level_values(1) == metric].mean(),\n",
    "            'Std': group[imput][group[imput].index.get_level_values(1) == metric].std()}, axis = 1)\n",
    "        for imput in performances_overall\n",
    "    }\n",
    "    mean = pd.concat(mean, axis = 1).swaplevel(0, axis = 1)\n",
    "    ax = mean.Mean.plot.barh(ax = ax, legend = False, xerr = 1.96 * difference.Std / np.sqrt(100), width = 0.7, ecolor = color, error_kw = {\"alpha\": 0.25, 'elinewidth': 3}, figsize = (3.2, 4.8))\n",
    "\n",
    "    # Remove bar and replace with dot\n",
    "    for i, thisbar in enumerate(ax.patches):\n",
    "        if i >= last:\n",
    "            thisbar.set(alpha = 0)\n",
    "            dot = ax.plot(thisbar.get_width(), thisbar.get_y() + thisbar.get_height() / 2, color = color, alpha = 0.7, marker = ('|' if name != 'Overall' else 'x'), markersize = 15, markeredgewidth=3)\n",
    "            last += 1\n",
    "\n",
    "    patches += dot\n",
    "    labels += [name]\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlim(0.25, 1.05)\n",
    "ax.legend(patches, labels, loc='upper left', bbox_to_anchor=(1.3, 1.04),\n",
    "        title = 'Imputation strategies', handletextpad = 0.5, handlelength = 1.0, columnspacing = -0.5,)\n",
    "ax.set_xlabel('Group-specific AUC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(performance, type = 'AUC', legend = True, colors = ['tab:orange', 'tab:blue', 'tab:gray'], alphas = None, overall_perf = None):\n",
    "    mean, ci, dots, dots_ci = {}, {}, {}, {}\n",
    "    for method in performance:\n",
    "        mean[method], ci[method], dots[method], dots_ci[method] = {}, {}, {}, {}\n",
    "        for group in performance[method].columns:\n",
    "            meth_group = performance[method][group]\n",
    "            meth_group = meth_group[meth_group.index.get_level_values('Metric') == type]\n",
    "\n",
    "            if overall_perf is not None:\n",
    "                meth_all = overall_perf[method][group]\n",
    "                meth_all = meth_all[meth_all.index.get_level_values('Metric') == type]\n",
    "                dots[method][group] = meth_all.mean()\n",
    "                dots_ci[method][group] = 1.96 * meth_all.std() / np.sqrt(len(meth_all))\n",
    "\n",
    "            mean[method][group] = meth_group.mean()\n",
    "            ci[method][group] = 1.96 * meth_group.std() / np.sqrt(len(meth_group))\n",
    "\n",
    "    mean, ci, dots, dots_ci = pd.DataFrame.from_dict(mean), pd.DataFrame.from_dict(ci), pd.DataFrame.from_dict(dots), pd.DataFrame.from_dict(dots_ci)\n",
    "    print(pd.DataFrame.from_dict({m: [\"{:.3f} ({:.3f})\".format(mean.loc[m].loc[i], ci.loc[m].loc[i]) for i in mean.columns] for m in mean.index}, columns = mean.columns, orient = 'index').to_latex())\n",
    "    ax = mean.plot.barh(xerr = ci, legend = legend, figsize = (7, 7), width = 0.75)\n",
    "    # Change colors\n",
    "    hatches = ['', 'ooo', 'xx', '//', '||', '***', '++']\n",
    "    for i, thisbar in enumerate(ax.patches):\n",
    "        c = list(plt_colors.to_rgba(colors[i % len(mean)]))\n",
    "        c[3] = 1 if alphas is None else alphas[i // len(mean)]\n",
    "        thisbar.set(edgecolor = '#eaeaf2', facecolor = c, linewidth = 1, hatch = hatches[i // len(mean)])\n",
    "\n",
    "        if overall_perf is not None:\n",
    "            y = thisbar.get_y() + thisbar.get_height() / 2\n",
    "            plt.plot([dots.values[i % len(mean)][i // len(mean)] - dots_ci.values[i % len(mean)][i // len(mean)], dots.values[i % len(mean)][i // len(mean)] + dots_ci.values[i % len(mean)][i // len(mean)]], [y, y], alpha = 0.5)\n",
    "            scatter = plt.scatter(dots.values[i % len(mean)][i // len(mean)], y, c = 'tab:gray', label = 'Population ' + type, alpha = 0.5, s = 100)\n",
    "        \n",
    "    plt.grid(alpha = 0.3)\n",
    "    \n",
    "    if type == 'AUC':\n",
    "        plt.xlim(0.2, 1.0)\n",
    "        plt.axvline(0.5, ls = ':', c = 'k', alpha = 0.5)\n",
    "    else:\n",
    "        plt.xlim(0., 0.8)\n",
    "    plt.xlabel(type)\n",
    "\n",
    "    if legend:\n",
    "        ncol = len(np.unique(colors))\n",
    "        patches = [ax.patches[i * len(mean) + j] for j in range(ncol) for i in range(len(mean.columns))][::-1]\n",
    "        labels = [''] * (len(patches) - len(mean.columns)) + mean.columns.tolist()[::-1] \n",
    "        \n",
    "        if dots is not None:\n",
    "            patches += [plt.scatter([], [], label = ' ', alpha = 0), scatter]\n",
    "            labels  += ['', 'Population ' + type]\n",
    "\n",
    "        ax.legend(patches, labels, loc='center left', bbox_to_anchor=(1, 0.5),\n",
    "            title = 'Imputation strategies', ncol = ncol, handletextpad = 0.5, handlelength = 1.0, columnspacing = -0.5,)\n",
    "\n",
    "display_result(performances_minority, colors = ['tab:blue', 'tab:blue', 'tab:blue'], alphas = [0.35, 0.35, 1, 1], overall_perf = performances_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('survival')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
